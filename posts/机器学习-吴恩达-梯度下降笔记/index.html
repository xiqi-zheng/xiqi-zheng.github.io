<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.101.0" /><meta name="theme-color" content="#fff" />
    <meta name="color-scheme" content="light dark">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>机器学习-吴恩达 梯度下降（笔记） | Chloe&#39;s universe</title>

    <link rel="stylesheet" href="/css/meme.min.cb908ad1a8df1bd29011213e51b9bc319db3c34cb763b7026cbcdb94d4ad8710.css"/>

    
    
        <script src="/js/meme.min.99f22ee6fbbc7d4f7bcbca12b4a602587827428bc0ffd9b706e347fdcca8d0c1.js"></script>

    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" /></noscript>

    <meta name="author" content="Chloe" /><meta name="description" content="什么是梯度下降？ 梯度下降是寻找目标函数最小化的方法。 比如在上一篇 单变量线性回归模型 文……" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="Chloe&#39;s universe" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="Chloe&#39;s universe" />
    <meta name="msapplication-starturl" content="../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
    <link rel="canonical" href="https://xiqi-zheng.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AC%94%E8%AE%B0/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "WebPage",
        "datePublished": "2022-07-07T00:00:00+00:00",
        "dateModified": "2023-03-18T12:23:05+08:00",
        "url": "https://xiqi-zheng.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AC%94%E8%AE%B0/",
        "name": "机器学习-吴恩达 梯度下降（笔记）",
        "description": "什么是梯度下降？ 梯度下降是寻找目标函数最小化的方法。 比如在上一篇 单变量线性回归模型 文……",
        "image": "https://xiqi-zheng.github.io/icons/apple-touch-icon.png",
        "license": "转载请保留本文作者以及本文链接",
        "publisher": {
            "@type": "Organization",
            "name": "Chloe's universe",
            "logo": {
                "@type": "ImageObject",
                "url": "https://xiqi-zheng.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://xiqi-zheng.github.io/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://xiqi-zheng.github.io/"
        }
    }
</script>

    

<meta name="twitter:card" content="summary" />


    



<meta property="og:title" content="机器学习-吴恩达 梯度下降（笔记）" />
<meta property="og:description" content="什么是梯度下降？ 梯度下降是寻找目标函数最小化的方法。 比如在上一篇 单变量线性回归模型 文……" />
<meta property="og:url" content="https://xiqi-zheng.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AC%94%E8%AE%B0/" />
<meta property="og:site_name" content="Chloe&#39;s universe" />
<meta property="og:locale" content="zh" /><meta property="og:image" content="https://xiqi-zheng.github.io/icons/apple-touch-icon.png" />
    <meta property="og:type" content="website" />


    
    

    

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" media="print" onload="this.media='all'" />
<noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" /></noscript>
</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            <div class="header-wrapper">
                <div class="header-inner single">
                    
    <div class="site-brand">
        
            <a href="/" class="brand">Chloe&#39;s universe</a>
        
    </div>

                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item active"><a href="/posts/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tech"><path d="M512 256c0 141.2-114.7 256-256 256C114.8 512 0 397.3 0 256S114.7 0 256 0s256 114.7 256 256zm-32 0c0-123.2-100.3-224-224-224C132.5 32 32 132.5 32 256s100.5 224 224 224 224-100.5 224-224zM160.9 124.6l86.9 37.1-37.1 86.9-86.9-37.1 37.1-86.9zm110 169.1l46.6 94h-14.6l-50-100-48.9 100h-14l51.1-106.9-22.3-9.4 6-14 68.6 29.1-6 14.3-16.5-7.1zm-11.8-116.3l68.6 29.4-29.4 68.3L230 246l29.1-68.6zm80.3 42.9l54.6 23.1-23.4 54.3-54.3-23.1 23.1-54.3z"/></svg><span class="menu-item-name">技术</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/random/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon random"><path d="M301.1 212c4.4 4.4 4.4 11.9 0 16.3l-9.7 9.7c-4.4 4.7-11.9 4.7-16.6 0l-10.5-10.5c-4.4-4.7-4.4-11.9 0-16.6l9.7-9.7c4.4-4.4 11.9-4.4 16.6 0l10.5 10.8zm-30.2-19.7c3-3 3-7.8 0-10.5-2.8-3-7.5-3-10.5 0-2.8 2.8-2.8 7.5 0 10.5 3.1 2.8 7.8 2.8 10.5 0zm-26 5.3c-3 2.8-3 7.5 0 10.2 2.8 3 7.5 3 10.5 0 2.8-2.8 2.8-7.5 0-10.2-3-3-7.7-3-10.5 0zm72.5-13.3c-19.9-14.4-33.8-43.2-11.9-68.1 21.6-24.9 40.7-17.2 59.8.8 11.9 11.3 29.3 24.9 17.2 48.2-12.5 23.5-45.1 33.2-65.1 19.1zm47.7-44.5c-8.9-10-23.3 6.9-15.5 16.1 7.4 9 32.1 2.4 15.5-16.1zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-66.2 42.6c2.5-16.1-20.2-16.6-25.2-25.7-13.6-24.1-27.7-36.8-54.5-30.4 11.6-8 23.5-6.1 23.5-6.1.3-6.4 0-13-9.4-24.9 3.9-12.5.3-22.4.3-22.4 15.5-8.6 26.8-24.4 29.1-43.2 3.6-31-18.8-59.2-49.8-62.8-22.1-2.5-43.7 7.7-54.3 25.7-23.2 40.1 1.4 70.9 22.4 81.4-14.4-1.4-34.3-11.9-40.1-34.3-6.6-25.7 2.8-49.8 8.9-61.4 0 0-4.4-5.8-8-8.9 0 0-13.8 0-24.6 5.3 11.9-15.2 25.2-14.4 25.2-14.4 0-6.4-.6-14.9-3.6-21.6-5.4-11-23.8-12.9-31.7 2.8.1-.2.3-.4.4-.5-5 11.9-1.1 55.9 16.9 87.2-2.5 1.4-9.1 6.1-13 10-21.6 9.7-56.2 60.3-56.2 60.3-28.2 10.8-77.2 50.9-70.6 79.7.3 3 1.4 5.5 3 7.5-2.8 2.2-5.5 5-8.3 8.3-11.9 13.8-5.3 35.2 17.7 24.4 15.8-7.2 29.6-20.2 36.3-30.4 0 0-5.5-5-16.3-4.4 27.7-6.6 34.3-9.4 46.2-9.1 8 3.9 8-34.3 8-34.3 0-14.7-2.2-31-11.1-41.5 12.5 12.2 29.1 32.7 28 60.6-.8 18.3-15.2 23-15.2 23-9.1 16.6-43.2 65.9-30.4 106 0 0-9.7-14.9-10.2-22.1-17.4 19.4-46.5 52.3-24.6 64.5 26.6 14.7 108.8-88.6 126.2-142.3 34.6-20.8 55.4-47.3 63.9-65 22 43.5 95.3 94.5 101.1 59z"/></svg><span class="menu-item-name">杂记</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/tags/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="icon tag"><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"/></svg><span class="menu-item-name">标签</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon user-circle"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><span class="menu-item-name">关于</span></a>
                </li>
            
        
            
                
                    
                    
                        <li class="menu-item">
                            <a id="theme-switcher" href="#"><span class="icon theme-icon-light">🌞</span><span class="icon theme-icon-dark">🌙</span></a>
                        </li>
                    
                
            
        
            
                
            
        
    </ul>
</nav>

                    
                </div>
            </div>
            

        
    </header>




            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-align="justify" data-type="posts" data-toc-num="true">

            <h1 class="post-title p-name">机器学习-吴恩达 梯度下降（笔记）</h1>

            

            
                
            

            
                

<div class="post-meta">
    
    
    
    
    
    
    
    
</div>

            

            <nav class="contents">
  <h2 id="contents" class="contents-title">目录</h2><ol class="toc">
    <li><a id="contents:什么是梯度下降" href="#什么是梯度下降">什么是梯度下降？</a></li>
    <li><a id="contents:梯度下降表达式" href="#梯度下降表达式">梯度下降表达式</a>
      <ol>
        <li>
          <ol>
            <li><a id="contents:表达式解释" href="#表达式解释">表达式解释</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a id="contents:梯度下降的直观理解" href="#梯度下降的直观理解">梯度下降的直观理解</a>
      <ol>
        <li><a id="contents:jw-靠近最小值的速度会逐渐变慢" href="#jw-靠近最小值的速度会逐渐变慢">J(w) 靠近最小值的速度会逐渐变慢</a></li>
      </ol>
    </li>
    <li><a id="contents:线性回归中的梯度下降" href="#线性回归中的梯度下降">线性回归中的梯度下降</a>
      <ol>
        <li>
          <ol>
            <li><a id="contents:求导过程" href="#求导过程">求导过程：</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav><div class="post-body e-content">
                <h2 id="什么是梯度下降"><a href="#什么是梯度下降" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:什么是梯度下降" class="headings">什么是梯度下降？</a></h2>
<p><strong>梯度下降是寻找目标函数最小化的方法。</strong></p>
<p>比如在上一篇</p>
<p><a href="https://blog.csdn.net/zxq1_/article/details/125470263" target="_blank" rel="noopener">单变量线性回归模型</a></p>
<p>文章中我们的目标是得到最拟合的<strong>单变量线性回归Function</strong>，也就是得到<strong>代价函数的最小值</strong>：min J(w,b) 。
那么如何得到呢？<strong>梯度下降法就可以通过不断迭代调整参数来寻找最合适的值。</strong>
<br/></p>
<h2 id="梯度下降表达式"><a href="#梯度下降表达式" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:梯度下降表达式" class="headings">梯度下降表达式</a></h2>
<p>我们还是用单变量线性回归模型中的J(w,b)来举例：</p>
<img src="https://img-blog.csdnimg.cn/b2b88a8367674b49a620b37c96a0e6fc.png" referrerpolicy='no-referrer'>
<p>我们<strong>持续不断地更新w和b</strong>，直到他们<strong>收敛</strong>，也就是计算后他们的值已经不会出现什么变动，那么我们就得到了局部（以下文章会解释为什么是局部）/ 全局最小值（<strong>只是收敛到极小值，而不是真正意义上的最小值。以下说的最小值，其实都是极小值</strong>）或是鞍点。</p>
<blockquote>
<p><strong>注意我们需要同步更新w和b，也就是w更新后，b更新时用的还是没有更新的w。</strong>
<img src="https://img-blog.csdnimg.cn/ad3b16a4b25a416fbf4873c8131b7cd6.png" referrerpolicy='no-referrer'></p>
</blockquote>
<h4 id="表达式解释"><a href="#表达式解释" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:表达式解释" class="headings">表达式解释</a></h4>
<p>这里的<strong>等号</strong>是一个<strong>赋值符号</strong>，而不是数学意义上的相等符号。
这里的<strong>Alpha</strong>是<strong>学习率（learning rate）</strong>，用来<strong>控制步长</strong>，也就是我们每一步的跨度，一定大于0，通常在0到1之间，在之后的文章中会详细讲解。
这里的最后一项是<strong>对于代价函数J(w,b)的偏导</strong>，用来<strong>控制方向</strong>。
<br/></p>
<h2 id="梯度下降的直观理解"><a href="#梯度下降的直观理解" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:梯度下降的直观理解" class="headings">梯度下降的直观理解</a></h2>
<p>为了更直观的观察梯度下降是如何让目标函数的最小化，我们假设<strong>只有一个参数的代价函数 J(w)</strong>，在上一章中我们知道J(w)是一个二次函数（quadratic function），也就是一个抛物线。</p>
<img src="https://img-blog.csdnimg.cn/5a33605e5c5e46e58800812614a1c478.png" referrerpolicy='no-referrer'>
<p>我们假设一个在抛物线上的初始点</p>
<img src="https://img-blog.csdnimg.cn/658c46a5e504483d8379f6ebbaaebcfe.png" referrerpolicy='no-referrer'>
<p>现在开始进行梯度下降：</p>
<img src="https://img-blog.csdnimg.cn/ffae7697ac3c479884345058b053093b.png" referrerpolicy='no-referrer'>
<p>这里的最后一项（对J(w)的求导）在图中就是这个点的斜率：</p>
<img src="https://img-blog.csdnimg.cn/13417285bd894a8394c527ecb4323573.png" referrerpolicy='no-referrer'>
<p>那么也就是<strong>w = w - $\alpha$ * 一个正数，由于alpha一定是正数，所以w减小了所以在图中的表现就是点向左移动</strong>，也就是向最小值靠近：</p>
<img src="https://img-blog.csdnimg.cn/f9ac1a9155ed4fae9ba7af17168f27da.png" referrerpolicy='no-referrer'>
<p>再来看另一个例子，当我们取的点在抛物线左边：</p>
<img src="https://img-blog.csdnimg.cn/8ecdc5da82104085937d4bff924a5442.png" referrerpolicy='no-referrer'>
<p>那么这个时候J(w)的求导就是负数，也就是一个负的斜率：</p>
<img src="https://img-blog.csdnimg.cn/9250ec8014384c04b2739d92e251a8c1.png" referrerpolicy='no-referrer'>
<p>那么现在<strong>w = w - $\alpha$ * 一个负数，w增大了，所以在图中的表现就是点向右移动</strong>，也在向最小值靠近：</p>
<img src="https://img-blog.csdnimg.cn/ecd1f382b61b4e33835531bfc2b6b200.png" referrerpolicy='no-referrer'>
<br/>
## 学习率（learning rate）
学习率的过大或过小都会造成一些影响：
**学习率过小：** 梯度下降的**速度会非常慢**，因为每次下降的步长都太小，虽然最终也能得到正确结果，但**会花费非常长的时间**。
**学习率过大：** 梯度下降的过多，会错过最小值，并且**无法收敛**。
<img src="https://img-blog.csdnimg.cn/66c120a53ec3401a99023e4ca521e1ca.png" referrerpolicy='no-referrer'>
<br/>
### 固定的学习率只能找到局部最小值
由于当 J(w) 的取值在局部最小值的时候，这个点的**斜率为0**，那么在下一次更新的时候，由于最后一项求导值为0，0乘上alpha还是0，所以**w的取值已经不会再改变**。
<img src="https://img-blog.csdnimg.cn/ebb88c9eb2144586ababfafb047437fb.png" referrerpolicy='no-referrer'>
<p><strong>到达哪个局部最小值取决于选择的起始点</strong></p>
<img src="https://img-blog.csdnimg.cn/33ba405f6f92457f85ba59b8b439dd37.png" referrerpolicy='no-referrer'>
<blockquote>
<p><strong>但在回归中使用平方代价函数时，代价函数没有也永远不会有多个局部最优解，也就是只有全局最优解。这种函数我们称为凸函数。</strong>
<br/>
<img src="https://img-blog.csdnimg.cn/43a127c1a71c475693036045d790a776.png" referrerpolicy='no-referrer'></p>
</blockquote>
<br/>
<h3 id="jw-靠近最小值的速度会逐渐变慢"><a href="#jw-靠近最小值的速度会逐渐变慢" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:jw-靠近最小值的速度会逐渐变慢" class="headings">J(w) 靠近最小值的速度会逐渐变慢</a></h3>
<p><strong>即使在 alpha（学习率）不变的情况下</strong>，随着 J(w) 的值向最小值靠近的时候，<strong>点的斜率会越来越小，所以w变化的速度也会逐渐变慢。</strong></p>
<img src="https://img-blog.csdnimg.cn/8341f01d09e54e67914966e703d47691.png" referrerpolicy='no-referrer'>
<br/>
<h2 id="线性回归中的梯度下降"><a href="#线性回归中的梯度下降" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:线性回归中的梯度下降" class="headings">线性回归中的梯度下降</a></h2>
<p>回顾上一章的内容：</p>
<img src="https://img-blog.csdnimg.cn/3911dd3c02f245f7b63619f487d86d73.png" referrerpolicy='no-referrer'>
<p><strong>在对J(w,b)进行求导后得到：</strong></p>
<img src="https://img-blog.csdnimg.cn/788314205f7e480bbf7f378eb8ed211f.png" referrerpolicy='no-referrer'>
<h4 id="求导过程"><a href="#求导过程" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:求导过程" class="headings">求导过程：</a></h4>
<p><strong>对w求导：</strong></p>
<img src="https://img-blog.csdnimg.cn/b6adbb0a733641959fe957d6940a67d0.jpeg" referrerpolicy='no-referrer'>
<p><strong>对b求导：</strong></p>
<img src="https://img-blog.csdnimg.cn/ccb34b513dcb404fa089f3c0a231e551.jpeg" referrerpolicy='no-referrer'>
<blockquote>
<p>重复对 w 和 b 的求导，直到它们收敛。我们就找到了 f(x) 的最小值。</p>
</blockquote>

            </div>

            


        </article>

        

        


        


        


        


        


        


        


        


        


    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            

        </div>
        <script>
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', function() {
                navigator.serviceWorker.register('\/sw.js');
            });
        }
    </script>


        








    <script src="https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js"></script>

<script>
    let imgNodes = document.querySelectorAll('div.post-body img');
    imgNodes = Array.from(imgNodes).filter(node => node.parentNode.tagName !== "A");

    mediumZoom(imgNodes, {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>






    </body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.101.0" /><meta name="theme-color" content="#fff" />
    <meta name="color-scheme" content="light dark">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>机器学习-吴恩达 多元线性回归（笔记） | Chloe&#39;s universe</title>

    <link rel="stylesheet" href="/css/meme.min.3309f6e1351b3684a21ae4c63462ddc28e81234a2d8b045d6b698ba332c2c681.css"/>

    
    
        <script src="/js/meme.min.99f22ee6fbbc7d4f7bcbca12b4a602587827428bc0ffd9b706e347fdcca8d0c1.js"></script>

    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" /></noscript>

    <meta name="author" content="Chloe" /><meta name="description" content="多类特征 在前几篇文章中介绍了单变量线性回归模型，但在现实生活中很多时候我们都不仅仅要……" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="Chloe&#39;s universe" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="Chloe&#39;s universe" />
    <meta name="msapplication-starturl" content="../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
    <link rel="canonical" href="https://xiqi-zheng.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AC%94%E8%AE%B0/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "datePublished": "2022-07-13T00:00:00+00:00",
        "dateModified": "2022-08-08T15:26:19+08:00",
        "url": "https://xiqi-zheng.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AC%94%E8%AE%B0/",
        "headline": "机器学习-吴恩达 多元线性回归（笔记）",
        "description": "多类特征 在前几篇文章中介绍了单变量线性回归模型，但在现实生活中很多时候我们都不仅仅要……",
        "inLanguage" : "zh-CN",
        "articleSection": "posts",
        "wordCount":  3155 ,
        "image": ["https://img-blog.csdnimg.cn/ce6aafd878aa4297a361d88fd7d4f699.png","https://img-blog.csdnimg.cn/562c50cbe99e417b8ba77d9cb25a130e.png","https://img-blog.csdnimg.cn/d04d316d7bae4a9599ea0f7a283a0769.png","https://img-blog.csdnimg.cn/4e0a973bda6c42cf8aa1175e6bc3266b.png","https://img-blog.csdnimg.cn/1a5ae4996afa47258658df5adc3d08b6.png","https://img-blog.csdnimg.cn/f14c0f9b1a2d403ca09fd845fb1cbab9.png","https://img-blog.csdnimg.cn/5b8faf68cb6e429ebe26c99e93a48e8d.png","https://img-blog.csdnimg.cn/9a05b40ca0a340cda0fce81487963ebd.png","https://img-blog.csdnimg.cn/399d3a5609784cc7be20bc31cdb07a39.png","https://img-blog.csdnimg.cn/89fdf443070f406eaf26093129d4b69a.png","https://img-blog.csdnimg.cn/e26337413049433798ae7c985af20f68.png","https://img-blog.csdnimg.cn/9f0f4a37a11d4bb3950d1c3bf7f9c58d.png","https://img-blog.csdnimg.cn/a6f163e899e5465fb980752e9965acdc.png","https://img-blog.csdnimg.cn/1832d79bdfea4207b80cc617a3788184.png","https://img-blog.csdnimg.cn/60c68d39a4a5419589cce453661465b3.png","https://img-blog.csdnimg.cn/8e644c4b00cc4f5783a86f9c7c517798.png","https://img-blog.csdnimg.cn/0405d0fbf6d74242a114868e5838ff93.png","https://img-blog.csdnimg.cn/d01e85f9acf5410cb5a9fcf7cdaa0304.png","https://img-blog.csdnimg.cn/c9a61fb44de14ee98dc269343a23a9c1.png","https://img-blog.csdnimg.cn/1056ae94dfd54ef2ba943871bec33d4f.png","https://img-blog.csdnimg.cn/0eb9baa18a4149608717403b9edf5eb5.png","https://img-blog.csdnimg.cn/aaacda5edad141f49cb5c030d9369fca.png","https://img-blog.csdnimg.cn/c821541089e04674aece6b19ad127cfa.png","https://img-blog.csdnimg.cn/bfbe56ae33a140bcb9b48dfc493af828.png","https://img-blog.csdnimg.cn/ef93eb3054b34d489e514dce27b009c1.png","https://img-blog.csdnimg.cn/5a3f858b4de44399b86a74c0ce3e915b.png","https://img-blog.csdnimg.cn/9c8defd742af4134a1241764ba97361b.png","https://img-blog.csdnimg.cn/4252b2d1283a4bd0a948e6db67e66217.png","https://img-blog.csdnimg.cn/76e67889d52c49b68daedf42144afaea.png","https://img-blog.csdnimg.cn/dbfe6ab6f1ed489692c54591816d3f9f.png","https://img-blog.csdnimg.cn/524bf3c4609749759c4b02213de55095.png","https://img-blog.csdnimg.cn/8c2ff71f56684437acaaf3cd1aafe82d.png","https://img-blog.csdnimg.cn/2675842148d04daf88c25e32413ed76c.png","https://img-blog.csdnimg.cn/bcaea748b94c46adb0c147b6a40e70a1.png","https://img-blog.csdnimg.cn/b4847a61e5f746659ce34015ce3bfeee.png","https://img-blog.csdnimg.cn/e667f4e4fc4d45c4ba18afa7c08adef6.png","https://img-blog.csdnimg.cn/8f674745aca54333a30a98dd56a57739.png","https://img-blog.csdnimg.cn/13605f67d240404ab0c39d7f74421682.png","https://img-blog.csdnimg.cn/facdb0bf942a4541b0bbaabe734679d1.png","https://img-blog.csdnimg.cn/6318132fbb4d4bcab47a6b458e9a6fe0.png","https://img-blog.csdnimg.cn/01574fad324b4301851c5d4e7589393c.png","https://img-blog.csdnimg.cn/3b840b89038b4f0d975e6ebb2da0d690.png","https://img-blog.csdnimg.cn/b49f56b4cc2e45d191fe1ae6dde15fc0.png","https://img-blog.csdnimg.cn/220b45d8f1a943818a52ab057bec96a7.png","https://img-blog.csdnimg.cn/ebcb56c2b1be46268ae6869eef643ae8.png","https://img-blog.csdnimg.cn/7340d7c5bc1d49b4a80bd22b29deddfb.png","https://img-blog.csdnimg.cn/5c9d3b62df194beda4a709c3a4ec9f09.png","https://img-blog.csdnimg.cn/48a2d72df103475d8d2f456235f7afbf.png","https://img-blog.csdnimg.cn/d12dcef0668943b5a108658992c5e203.png"],
        "author": {
            "@type": "Person",
            "description": "Let there be light",
            "email": "1647656181@qq.com",
            "image": "https://xiqi-zheng.github.io/icons/apple-touch-icon.png",
            "url": "https://io-oi.me/",
            "name": "Chloe"
        },
        "license": "转载请保留本文作者以及本文链接",
        "publisher": {
            "@type": "Organization",
            "name": "Chloe's universe",
            "logo": {
                "@type": "ImageObject",
                "url": "https://xiqi-zheng.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://xiqi-zheng.github.io/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://xiqi-zheng.github.io/"
        }
    }
</script>

    

<meta name="twitter:card" content="summary_large_image" />



    



<meta property="og:title" content="机器学习-吴恩达 多元线性回归（笔记）" />
<meta property="og:description" content="多类特征 在前几篇文章中介绍了单变量线性回归模型，但在现实生活中很多时候我们都不仅仅要……" />
<meta property="og:url" content="https://xiqi-zheng.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AC%94%E8%AE%B0/" />
<meta property="og:site_name" content="Chloe&#39;s universe" />
<meta property="og:locale" content="zh" /><meta property="og:image" content="https://img-blog.csdnimg.cn/ce6aafd878aa4297a361d88fd7d4f699.png" />
<meta property="og:type" content="article" />
    <meta property="article:published_time" content="2022-07-13T00:00:00&#43;00:00" />
    <meta property="article:modified_time" content="2022-08-08T15:26:19&#43;08:00" />
    
    <meta property="article:section" content="posts" />



    
    

    

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" media="print" onload="this.media='all'" />
<noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@700&amp;text=reuixiy&amp;display=swap" /></noscript>
</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            <div class="header-wrapper">
                <div class="header-inner single">
                    
    <div class="site-brand">
        
            <a href="/" class="brand">Chloe&#39;s universe</a>
        
    </div>

                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item active"><a href="/posts/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tech"><path d="M512 256c0 141.2-114.7 256-256 256C114.8 512 0 397.3 0 256S114.7 0 256 0s256 114.7 256 256zm-32 0c0-123.2-100.3-224-224-224C132.5 32 32 132.5 32 256s100.5 224 224 224 224-100.5 224-224zM160.9 124.6l86.9 37.1-37.1 86.9-86.9-37.1 37.1-86.9zm110 169.1l46.6 94h-14.6l-50-100-48.9 100h-14l51.1-106.9-22.3-9.4 6-14 68.6 29.1-6 14.3-16.5-7.1zm-11.8-116.3l68.6 29.4-29.4 68.3L230 246l29.1-68.6zm80.3 42.9l54.6 23.1-23.4 54.3-54.3-23.1 23.1-54.3z"/></svg><span class="menu-item-name">技术</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/random/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon random"><path d="M301.1 212c4.4 4.4 4.4 11.9 0 16.3l-9.7 9.7c-4.4 4.7-11.9 4.7-16.6 0l-10.5-10.5c-4.4-4.7-4.4-11.9 0-16.6l9.7-9.7c4.4-4.4 11.9-4.4 16.6 0l10.5 10.8zm-30.2-19.7c3-3 3-7.8 0-10.5-2.8-3-7.5-3-10.5 0-2.8 2.8-2.8 7.5 0 10.5 3.1 2.8 7.8 2.8 10.5 0zm-26 5.3c-3 2.8-3 7.5 0 10.2 2.8 3 7.5 3 10.5 0 2.8-2.8 2.8-7.5 0-10.2-3-3-7.7-3-10.5 0zm72.5-13.3c-19.9-14.4-33.8-43.2-11.9-68.1 21.6-24.9 40.7-17.2 59.8.8 11.9 11.3 29.3 24.9 17.2 48.2-12.5 23.5-45.1 33.2-65.1 19.1zm47.7-44.5c-8.9-10-23.3 6.9-15.5 16.1 7.4 9 32.1 2.4 15.5-16.1zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-66.2 42.6c2.5-16.1-20.2-16.6-25.2-25.7-13.6-24.1-27.7-36.8-54.5-30.4 11.6-8 23.5-6.1 23.5-6.1.3-6.4 0-13-9.4-24.9 3.9-12.5.3-22.4.3-22.4 15.5-8.6 26.8-24.4 29.1-43.2 3.6-31-18.8-59.2-49.8-62.8-22.1-2.5-43.7 7.7-54.3 25.7-23.2 40.1 1.4 70.9 22.4 81.4-14.4-1.4-34.3-11.9-40.1-34.3-6.6-25.7 2.8-49.8 8.9-61.4 0 0-4.4-5.8-8-8.9 0 0-13.8 0-24.6 5.3 11.9-15.2 25.2-14.4 25.2-14.4 0-6.4-.6-14.9-3.6-21.6-5.4-11-23.8-12.9-31.7 2.8.1-.2.3-.4.4-.5-5 11.9-1.1 55.9 16.9 87.2-2.5 1.4-9.1 6.1-13 10-21.6 9.7-56.2 60.3-56.2 60.3-28.2 10.8-77.2 50.9-70.6 79.7.3 3 1.4 5.5 3 7.5-2.8 2.2-5.5 5-8.3 8.3-11.9 13.8-5.3 35.2 17.7 24.4 15.8-7.2 29.6-20.2 36.3-30.4 0 0-5.5-5-16.3-4.4 27.7-6.6 34.3-9.4 46.2-9.1 8 3.9 8-34.3 8-34.3 0-14.7-2.2-31-11.1-41.5 12.5 12.2 29.1 32.7 28 60.6-.8 18.3-15.2 23-15.2 23-9.1 16.6-43.2 65.9-30.4 106 0 0-9.7-14.9-10.2-22.1-17.4 19.4-46.5 52.3-24.6 64.5 26.6 14.7 108.8-88.6 126.2-142.3 34.6-20.8 55.4-47.3 63.9-65 22 43.5 95.3 94.5 101.1 59z"/></svg><span class="menu-item-name">杂记</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/tags/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="icon tag"><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"/></svg><span class="menu-item-name">标签</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon user-circle"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><span class="menu-item-name">关于</span></a>
                </li>
            
        
            
                
                    
                    
                        <li class="menu-item">
                            <a id="theme-switcher" href="#"><span class="icon theme-icon-light">🌞</span><span class="icon theme-icon-dark">🌙</span></a>
                        </li>
                    
                
            
        
            
                
            
        
    </ul>
</nav>

                    
                </div>
            </div>
            

        
    </header>




            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-align="justify" data-type="posts" data-toc-num="true">

            <h1 class="post-title p-name">机器学习-吴恩达 多元线性回归（笔记）</h1>

            

            
                
            

            
                

<div class="post-meta">
    
    
    
    
    
    
    
    
</div>

            

            <nav class="contents">
  <h2 id="contents" class="contents-title">目录</h2><ol class="toc">
    <li><a id="contents:新的符号notation" href="#新的符号notation">新的符号（Notation）</a></li>
    <li><a id="contents:多元线性回归模型" href="#多元线性回归模型">多元线性回归模型</a>
      <ol>
        <li><a id="contents:引入符号重写表达式" href="#引入符号重写表达式">引入符号重写表达式</a></li>
      </ol>
    </li>
  </ol>

  <ol>
    <li><a id="contents:向量化和不向量化的区别" href="#向量化和不向量化的区别">向量化和不向量化的区别</a></li>
  </ol>

  <ol>
    <li><a id="contents:梯度下降公式" href="#梯度下降公式">梯度下降公式：</a></li>
  </ol>

  <ol>
    <li><a id="contents:如何选择w的值" href="#如何选择w的值">如何选择w的值</a>
      <ol>
        <li><a id="contents:参数w的选择与梯度下降的关系" href="#参数w的选择与梯度下降的关系">参数(w)的选择与梯度下降的关系</a></li>
        <li><a id="contents:进行特征缩放" href="#进行特征缩放">进行特征缩放</a>
          <ol>
            <li><a id="contents:除以各自的最大值" href="#除以各自的最大值">除以各自的最大值</a></li>
            <li><a id="contents:均值归一化" href="#均值归一化">均值归一化</a></li>
            <li><a id="contents:离差归一化z-score-归一化" href="#离差归一化z-score-归一化">离差归一化（Z-score 归一化）</a></li>
            <li><a id="contents:什么时候需要进行特征缩放" href="#什么时候需要进行特征缩放">什么时候需要进行特征缩放</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>

  <ol>
    <li><a id="contents:使用学习曲线判断推荐" href="#使用学习曲线判断推荐">使用学习曲线判断（推荐）</a></li>
    <li><a id="contents:使用自动收敛测试" href="#使用自动收敛测试">使用自动收敛测试</a>
      <ol>
        <li><a id="contents:学习率的选择" href="#学习率的选择">学习率的选择</a>
          <ol>
            <li><a id="contents:尝试选择学习率" href="#尝试选择学习率">尝试选择学习率</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav><div class="post-body e-content">
                <h1 id="多类特征"><a href="#多类特征" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:多类特征" class="headings">多类特征</a></h1>
<p>在前几篇文章中介绍了单变量线性回归模型，但在现实生活中很多时候我们都不仅仅要考虑一个特征，而是<strong>考虑多个特征</strong>。</p>
<h2 id="新的符号notation"><a href="#新的符号notation" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:新的符号notation" class="headings">新的符号（Notation）</a></h2>
<ul>
<li>我们用x1, x2, x3, ... , xn 来表示每一个特征（features / variables）</li>
<li>n 是特征的数量
<img src="https://img-blog.csdnimg.cn/ce6aafd878aa4297a361d88fd7d4f699.png" alt="在这里插入图片描述">
<strong>这里的 j 可以理解为某一列，(i) 可以理解为某一行</strong>
<br/></li>
</ul>
<h2 id="多元线性回归模型"><a href="#多元线性回归模型" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:多元线性回归模型" class="headings">多元线性回归模型</a></h2>
<p>回顾之前的单变量线性回归模型：<img src="https://img-blog.csdnimg.cn/562c50cbe99e417b8ba77d9cb25a130e.png" alt="在这里插入图片描述">
那么当有多个变量时：<img src="https://img-blog.csdnimg.cn/d04d316d7bae4a9599ea0f7a283a0769.png" alt="在这里插入图片描述">
上图举了预测房子价格的例子，<strong>x1是房子的面积，x2是房子卧室的数量，x3是房子的层数，x4是房子的年龄</strong>（也就是被住了多少年），最后一个<strong>常数80则是基本价格</strong>，也就是假设这个房子没有面积，没有卧室，没有地板也没有年龄的时候的价格。前面的w1 ... w4 可以理解为x1 ... x2的权重，比如一个房子每多一个房间，价格可能就增加四千磅。</p>
<p>如果有更多变量：<img src="https://img-blog.csdnimg.cn/4e0a973bda6c42cf8aa1175e6bc3266b.png" alt="在这里插入图片描述"></p>
<h3 id="引入符号重写表达式"><a href="#引入符号重写表达式" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:引入符号重写表达式" class="headings">引入符号重写表达式</a></h3>
<p>我们把w定义为一个数字列表，列出参数 w1 ... wn ，可以理解称一个行向量，且是个矢量：<img src="https://img-blog.csdnimg.cn/1a5ae4996afa47258658df5adc3d08b6.png" alt="在这里插入图片描述">
而 b 依旧是一个常量。
然后我们把x也写成一个行向量（列表），它列出了所有特性x1 ... xn：<img src="https://img-blog.csdnimg.cn/f14c0f9b1a2d403ca09fd845fb1cbab9.png" alt="在这里插入图片描述">
那么我们就能把之前的表达式改写成：
<img src="https://img-blog.csdnimg.cn/5b8faf68cb6e429ebe26c99e93a48e8d.png" alt="在这里插入图片描述"></p>
<p><strong>我们把这个模型称为多元线性回归（multiple linear regression），而不是多元回归（multivariate regression）。</strong></p>
<h1 id="向量化"><a href="#向量化" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:向量化" class="headings">向量化</a></h1>
<p>在线性代数中，向量的索引是从1开始的，而在python的NumPy库中向量的索引是从0开始的。基本上所有的编程语言数组的下标都是从0开始计数的。<img src="https://img-blog.csdnimg.cn/9a05b40ca0a340cda0fce81487963ebd.png" alt="在这里插入图片描述">
上图中的np.array() 其实是调用了NumPy库中的方法，创建了一个数组（array）。</p>
<h2 id="向量化和不向量化的区别"><a href="#向量化和不向量化的区别" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:向量化和不向量化的区别" class="headings">向量化和不向量化的区别</a></h2>
<p><strong>不向量化：</strong>
我们需要手动地将每一项写出来，如果特征数量	少还可以接受，如果特征数量是10个甚至10000
个的时候，这种方法显然不适用：<img src="https://img-blog.csdnimg.cn/399d3a5609784cc7be20bc31cdb07a39.png" alt="在这里插入图片描述">
或者我们可以稍微再简化,用for循环来循环计算：<img src="https://img-blog.csdnimg.cn/89fdf443070f406eaf26093129d4b69a.png" alt="在这里插入图片描述">
<strong>向量化：</strong>
我们只需要写一行很短的表达式，和一行很短的代码就可以了：<img src="https://img-blog.csdnimg.cn/e26337413049433798ae7c985af20f68.png" alt="在这里插入图片描述">
其实除了这种显而易见的好处之外，使用向量化还能提高计算的速度。因为python在计算np.dot(w,x) 的时候，会<strong>使用并行计算</strong>，让计算效率提高。
<strong>使用for循环时：</strong>
计算机会先计算完前一个循环中的结果，再计算下一个循环：<img src="https://img-blog.csdnimg.cn/9f0f4a37a11d4bb3950d1c3bf7f9c58d.png" alt="在这里插入图片描述">
<strong>而使用np.dot()时：</strong>
计算机会同时计算w[j] * x[j] 这一步，最后再把它们加在一起：<img src="https://img-blog.csdnimg.cn/a6f163e899e5465fb980752e9965acdc.png" alt="在这里插入图片描述"></p>
<h1 id="多元线性回归的梯度下降法"><a href="#多元线性回归的梯度下降法" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:多元线性回归的梯度下降法" class="headings">多元线性回归的梯度下降法</a></h1>
<p><strong>代价函数（Cost Function）：</strong> <img src="https://img-blog.csdnimg.cn/1832d79bdfea4207b80cc617a3788184.png" alt="在这里插入图片描述"></p>
<h2 id="梯度下降公式"><a href="#梯度下降公式" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:梯度下降公式" class="headings">梯度下降公式：</a></h2>
<p><img src="https://img-blog.csdnimg.cn/60c68d39a4a5419589cce453661465b3.png" alt="在这里插入图片描述">
<strong>在单变量时：</strong>
<img src="https://img-blog.csdnimg.cn/8e644c4b00cc4f5783a86f9c7c517798.png" alt="在这里插入图片描述">
在多变量时，也就是n &gt;= 2时：<img src="https://img-blog.csdnimg.cn/0405d0fbf6d74242a114868e5838ff93.png" alt="在这里插入图片描述"></p>
<p><strong>注意更新w时，最后一项不是x(i) 了，而是x1(i) ... xn(i)。</strong></p>
<br/>
<h1 id="特征缩放"><a href="#特征缩放" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:特征缩放" class="headings">特征缩放</a></h1>
<h2 id="如何选择w的值"><a href="#如何选择w的值" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:如何选择w的值" class="headings">如何选择w的值</a></h2>
<p>我们假设要预测一个房子的价值，但我们现在只考虑它的size（面积）为<strong>x1</strong>，和bedrooms（卧室的数量）为<strong>x2</strong> 。<strong>x1的范围是300 - 2000；x2的范围是0-5。</strong><img src="https://img-blog.csdnimg.cn/d01e85f9acf5410cb5a9fcf7cdaa0304.png" alt="在这里插入图片描述">
假设现在：<img src="https://img-blog.csdnimg.cn/c9a61fb44de14ee98dc269343a23a9c1.png" alt="在这里插入图片描述"></p>
<ol>
<li>第一种情况：我们让 w1 = 50，w2 = 0.1，b = 50：<img src="https://img-blog.csdnimg.cn/1056ae94dfd54ef2ba943871bec33d4f.png" alt="在这里插入图片描述">
结果与我们的training example完全不拟合。</li>
<li>第二种情况：我们让w1 = 0.1, w2 = 50, b = 50：<img src="https://img-blog.csdnimg.cn/0eb9baa18a4149608717403b9edf5eb5.png" alt="在这里插入图片描述">
这时候的结果就非常拟合我们的training example。</li>
</ol>
<p><strong>总结：当某一个特征的可能值范围很大的时候，比如面积，一个好的模型更有可能选择一个相对较小的w值，比如0.1。反之，当一个特征的可能值很小时，比如卧室的数量，那么它w值可能就会比较大，比如50。</strong>
<br/></p>
<h3 id="参数w的选择与梯度下降的关系"><a href="#参数w的选择与梯度下降的关系" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:参数w的选择与梯度下降的关系" class="headings">参数(w)的选择与梯度下降的关系</a></h3>
<p>让我们来看看x1和x2的散点图（scatter plot）和代价函数的等高线图（contour plot）：
<img src="https://img-blog.csdnimg.cn/aaacda5edad141f49cb5c030d9369fca.png" alt="在这里插入图片描述">
在散点图中，我们发现横坐标的取值范围很大，而纵坐标的取值范围很小。
在代价函数的等高线图中，我们发现是一个椭圆形，这是因为<strong>w1的取值只要稍微变动一点点，就会对价格影响非常大</strong>，因为x1（房子的面积）的取值比较大；<strong>而w2需要变动的比较大才会对价格造成影响</strong>，因为x2（卧室数量）的取值比较小。
<img src="https://img-blog.csdnimg.cn/c821541089e04674aece6b19ad127cfa.png" alt="在这里插入图片描述">
在这种情况下，当我们试图进行梯度下降时，因为我们的训练数据的轮廓很<strong>高和瘦</strong>，这会导致在寻找最小值的时候会<strong>来回跳动很长时间</strong>：<img src="https://img-blog.csdnimg.cn/bfbe56ae33a140bcb9b48dfc493af828.png" alt="在这里插入图片描述">
<strong>所以我们希望找到一种方式让特征们（features）可以缩放到差不多的取值范围，达到这样：</strong><img src="https://img-blog.csdnimg.cn/ef93eb3054b34d489e514dce27b009c1.png" alt="在这里插入图片描述">
<br/></p>
<h3 id="进行特征缩放"><a href="#进行特征缩放" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:进行特征缩放" class="headings">进行特征缩放</a></h3>
<p>现在特征的取值范围是：<img src="https://img-blog.csdnimg.cn/5a3f858b4de44399b86a74c0ce3e915b.png" alt="在这里插入图片描述"></p>
<h4 id="除以各自的最大值"><a href="#除以各自的最大值" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:除以各自的最大值" class="headings">除以各自的最大值</a></h4>
<p>现在我们把它们<strong>除以各自的最大值</strong>：<img src="https://img-blog.csdnimg.cn/9c8defd742af4134a1241764ba97361b.png" alt="在这里插入图片描述">
把它们放到图上就会变成：<img src="https://img-blog.csdnimg.cn/4252b2d1283a4bd0a948e6db67e66217.png" alt="在这里插入图片描述"></p>
<h4 id="均值归一化"><a href="#均值归一化" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:均值归一化" class="headings">均值归一化</a></h4>
<p>除了除以最大值，我们还可以做均值归一化让它们都以0为中心：
我们把每个<strong>x1减去所有x1的均值</strong>，然后<strong>除以它们的取值范围的差值</strong>，x2,x3 .. xn 同理，比如：<img src="https://img-blog.csdnimg.cn/76e67889d52c49b68daedf42144afaea.png" alt="在这里插入图片描述">
在图上它们是这样的：<img src="https://img-blog.csdnimg.cn/dbfe6ab6f1ed489692c54591816d3f9f.png" alt="在这里插入图片描述">
<strong>之前它们只有大于0的值，现在它们有负数和正数，但通常在-1 和 +1 之间。</strong>
<br/></p>
<h4 id="离差归一化z-score-归一化"><a href="#离差归一化z-score-归一化" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:离差归一化z-score-归一化" class="headings">离差归一化（Z-score 归一化）</a></h4>
<p>我们还可以使用Z-score归一化来进行特征缩放：
在这之前我们除了需要<strong>计算平均数</strong>，还需要计算<strong>标准差（standard deviation）</strong> 。
我们把<strong>每个x1 减去 所有x1的平均数，然后除以x1的标准差</strong>，x2 ... xn 同理，比如：<img src="https://img-blog.csdnimg.cn/524bf3c4609749759c4b02213de55095.png" alt="在这里插入图片描述">
在图上的表现为：<img src="https://img-blog.csdnimg.cn/8c2ff71f56684437acaaf3cd1aafe82d.png" alt="在这里插入图片描述"></p>
<h4 id="什么时候需要进行特征缩放"><a href="#什么时候需要进行特征缩放" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:什么时候需要进行特征缩放" class="headings">什么时候需要进行特征缩放</a></h4>
<p>当x的取值在以下范围中都是可以接受的：
<img src="https://img-blog.csdnimg.cn/2675842148d04daf88c25e32413ed76c.png" alt="在这里插入图片描述">
但如果取值范围过大或者过小，那么就需要进行特征缩放，比如：<img src="https://img-blog.csdnimg.cn/bcaea748b94c46adb0c147b6a40e70a1.png" alt="在这里插入图片描述">
<strong>正常情况下，进行特征缩放几乎没有什么坏处</strong>，所以在疑惑自己是否需要进行特征缩放的时候，就尝试下特征缩放吧。
<br/></p>
<h1 id="检查梯度下降是否收敛"><a href="#检查梯度下降是否收敛" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:检查梯度下降是否收敛" class="headings">检查梯度下降是否收敛</a></h1>
<p>我们的目的：找到最小的J(w,b) <img src="https://img-blog.csdnimg.cn/b4847a61e5f746659ce34015ce3bfeee.png" alt="在这里插入图片描述"></p>
<h2 id="使用学习曲线判断推荐"><a href="#使用学习曲线判断推荐" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:使用学习曲线判断推荐" class="headings">使用学习曲线判断（推荐）</a></h2>
<p>一种方式保证梯度下降正常进行是画出一个学习曲线（learning curve），其纵轴是J(w,b)，横轴是迭代的次数：
<img src="https://img-blog.csdnimg.cn/e667f4e4fc4d45c4ba18afa7c08adef6.png" alt="在这里插入图片描述">
判断梯度下降是否正常运行：</p>
<ol>
<li>学习曲线是否在每一次迭代后持续下降</li>
<li>学习曲线在经过了一定次数的迭代后是否收敛（converge）到某一个值</li>
<li>学习曲线是否出现上下浮动，而不是平稳的下降
<br/></li>
</ol>
<h2 id="使用自动收敛测试"><a href="#使用自动收敛测试" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:使用自动收敛测试" class="headings">使用自动收敛测试</a></h2>
<ul>
<li>我们首先设定一个 &quot;epsilon&quot; 的值</li>
<li>如果J(w,b)的下降浮动小于等于 &quot;epsilon&quot;</li>
<li>就判定它收敛。<img src="https://img-blog.csdnimg.cn/8f674745aca54333a30a98dd56a57739.png" alt="在这里插入图片描述">
但是找到合适的 &quot;epsilon&quot; 值是相当困难的，所以<strong>推荐使用学习曲线</strong>，能够更加直观地看出梯度下降是否在正常进行中。
<br/></li>
</ul>
<h3 id="学习率的选择"><a href="#学习率的选择" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:学习率的选择" class="headings">学习率的选择</a></h3>
<p>我们可以通过学习曲线来看出，学习率的选择是否合适。
比如出现：
<img src="https://img-blog.csdnimg.cn/13605f67d240404ab0c39d7f74421682.png" alt="在这里插入图片描述">
出现上图的时候，通常是<strong>代码中有bug或者是学习率过大</strong>导致的。比如下图这种状况，学习率过大导致错过了最小值。
<img src="https://img-blog.csdnimg.cn/facdb0bf942a4541b0bbaabe734679d1.png" alt="在这里插入图片描述">
所以这个时候，我们可以选择一个非常非常小的alpha值，来检查是否在每次迭代后，我们的J(w,b）的值在持续下降。
如果还是没有下降，那么就说明可能是我们的代码中有bug，比如本来应该是:<img src="https://img-blog.csdnimg.cn/6318132fbb4d4bcab47a6b458e9a6fe0.png" alt="在这里插入图片描述">
但是却写成了：
<img src="https://img-blog.csdnimg.cn/01574fad324b4301851c5d4e7589393c.png" alt="在这里插入图片描述">
这个时候的学习曲线可能会变成这样：<img src="https://img-blog.csdnimg.cn/3b840b89038b4f0d975e6ebb2da0d690.png" alt="在这里插入图片描述"></p>
<h4 id="尝试选择学习率"><a href="#尝试选择学习率" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:尝试选择学习率" class="headings">尝试选择学习率</a></h4>
<ul>
<li>我们可以先选择一个非常小的alpha值（学习率）比如0.001</li>
<li>然后再选择一个相对较大的学习率，比如1，来查看是否出现学习曲线上下浮动的情况</li>
<li>然后再逐步地在最小的alpha的基础上一点点地增加，或者在较大的alpha值的基础上一点点地减少<img src="https://img-blog.csdnimg.cn/b49f56b4cc2e45d191fe1ae6dde15fc0.png" alt="在这里插入图片描述"></li>
</ul>
<h1 id="特征工程"><a href="#特征工程" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:特征工程" class="headings">特征工程</a></h1>
<p>假设现在我们要预估一个房子的价格，这个房子有两个特征，<strong>x1 是房子正面的长度</strong>（frontage），<strong>x2是房子的宽度（depth）</strong>。那么现在我们想要添加一个新的特征，那么我们可以选择<strong>x3为：
面积 = 长度 * 宽度</strong><img src="https://img-blog.csdnimg.cn/220b45d8f1a943818a52ab057bec96a7.png" alt="在这里插入图片描述">
处理一个新特征是特征工程的一个例子，这告诉我们<strong>设计新特征</strong>的时候，可以<strong>通过转换或结合原始特征</strong>。这样可能可以设计出更好的学习模型。<img src="https://img-blog.csdnimg.cn/ebcb56c2b1be46268ae6869eef643ae8.png" alt="在这里插入图片描述"></p>
<h1 id="多项式回归polynomial-regression"><a href="#多项式回归polynomial-regression" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href="#contents:多项式回归polynomial-regression" class="headings">多项式回归（polynomial regression）</a></h1>
<p>我们利用多元线性回归和特征工程的思想，提出一种新的 <strong>多项式回归算法</strong>
<img src="https://img-blog.csdnimg.cn/7340d7c5bc1d49b4a80bd22b29deddfb.png" alt="在这里插入图片描述">
上图是用房子的面积来预测房子的价格的数据，显然它不能有一个简单的直线来拟合。</p>
<p><strong>这时候我们就需要用到多项式函数：</strong>
我们可以选择<strong>二次函数</strong>，但显然这里也不适合用二次函数，因为二次函数最后会下降，但我们并不认为房子的面积变大时，价格会下降。<img src="https://img-blog.csdnimg.cn/5c9d3b62df194beda4a709c3a4ec9f09.png" alt="在这里插入图片描述"></p>
<p>所以我们可以进而选择<strong>三次函数</strong>，它能更好地拟合这些数据：<img src="https://img-blog.csdnimg.cn/48a2d72df103475d8d2f456235f7afbf.png" alt="在这里插入图片描述">
但在这个时候，<strong>特征缩放（feature scaling）就显得尤为重要</strong>，因为size的三次方会变得非常的大，这叫对我们选择w的值造成很大的困难，也会让计算变得更加复杂。</p>
<p>另一种更合理的替代方法是<strong>取尺寸的平方和立方</strong>：<img src="https://img-blog.csdnimg.cn/d12dcef0668943b5a108658992c5e203.png" alt="在这里插入图片描述">
<strong>如何选择最为合适的features会在后面的课程中介绍。</strong></p>

            </div>

            


        </article>

        

        


        


        


        


        


        
    <footer class="minimal-footer">
        
            <div class="post-tag"><a href="/tags/machine-learning/" rel="tag" class="post-tag-link">#machine-learning</a></div>
        
        
        
            
        
    </footer>



        


        


        


    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            

        </div>
        <script>
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', function() {
                navigator.serviceWorker.register('\/sw.js');
            });
        }
    </script>


        








    <script src="https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js"></script>

<script>
    let imgNodes = document.querySelectorAll('div.post-body img');
    imgNodes = Array.from(imgNodes).filter(node => node.parentNode.tagName !== "A");

    mediumZoom(imgNodes, {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>






    </body>
</html>
